{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import models, corpora, similarities\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"tiantian_news_cutforsearch.csv\")\n",
    "data = pd.read_csv(\"raw_data/tiantian_news_cut.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data.loc[data[\"cutwords\"].isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories():\n",
    "    input_file = \"conf/categories.txt\"\n",
    "    with open(input_file, \"rb\") as f:\n",
    "        result = f.readlines()\n",
    "        result = [x.strip('\\n').decode('utf8') for x in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = get_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4)\n(422067, 4)\n"
     ]
    }
   ],
   "source": [
    "sample_num = 5000\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "for cat in categories:\n",
    "    data_onecat = data[data[\"category\"] == cat.encode(\"utf8\")]\n",
    "    train_onecat = data_onecat.sample(n=sample_num)\n",
    "    test_onecat = data_onecat.drop(train_onecat.index)\n",
    "    train = pd.concat([train, train_onecat])\n",
    "    test = pd.concat([test, test_onecat])\n",
    "# test = test.sample(n=10000)\n",
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = train[\"cutwords\"].tolist()\n",
    "train_texts = [[word for word in document.split()] for document in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = test[\"cutwords\"].tolist()\n",
    "test_texts = [[word for word in document.split()] for document in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:07,884 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:12,961 : INFO : adding document #10000 to Dictionary(142380 unique tokens: [u'\\u4e0d\\u5b66\\u65e0\\u672f', u'\\u4e66\\u6bb5', u'\\u5c97\\u4e0a', u'\\u6d2a\\u6b66', u'\\u51fb\\u4e2d\\u76ee\\u6807']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:20,096 : INFO : adding document #20000 to Dictionary(248997 unique tokens: [u'\\u535a\\u878d', u'\\u5e94\\u5b66', u'\\u4e66\\u6bb5', u'\\u94c5\\u950c', u'\\u5c97\\u4e0a']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:27,756 : INFO : adding document #30000 to Dictionary(336134 unique tokens: [u'\\u535a\\u878d', u'TouchSense', u'\\u5e94\\u5b66', u'\\u4e66\\u6bb5', u'\\u94c5\\u950c']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:33,620 : INFO : adding document #40000 to Dictionary(405378 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u535a\\u878d', u'\\u5357\\u53ec', u'\\u4e66\\u6bb5', u'GF45mmf2']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:40,801 : INFO : adding document #50000 to Dictionary(484814 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u535a\\u878d', u'\\u5357\\u53ec', u'\\u4e66\\u6bb5', u'\\u4fee\\u817f\\u578b']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:51,115 : INFO : adding document #60000 to Dictionary(555830 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u535a\\u878d', u'\\u5357\\u53ec', u'\\u4e66\\u6bb5', u'\\u5f20\\u7fce']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:12:57,183 : INFO : adding document #70000 to Dictionary(598305 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u535a\\u878d', u'\\u5357\\u53ec', u'\\u4e66\\u6bb5', u'360.00']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:13:06,126 : INFO : adding document #80000 to Dictionary(707158 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u5434\\u6d69\\u5eb7', u'\\u4e66\\u6bb5', u'RogerWicker', u'\\u8003\\u5bdf\\u961f\\u5458']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:13:09,247 : INFO : adding document #90000 to Dictionary(724640 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u5434\\u6d69\\u5eb7', u'\\u4e66\\u6bb5', u'RogerWicker', u'\\u8003\\u5bdf\\u961f\\u5458']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:13:16,836 : INFO : built Dictionary(767562 unique tokens: [u'\\u85e4\\u6912\\u6cb9', u'\\u6e34\\u559c', u'\\u5434\\u6d69\\u5eb7', u'\\u4e66\\u6bb5', u'RogerWicker']...) from 100000 documents (total 31533597 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [dictionary.doc2bow(text) for text in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [dictionary.doc2bow(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:04,727 : INFO : collecting document frequencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:04,758 : INFO : PROGRESS: processing document #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:05,824 : INFO : PROGRESS: processing document #10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:06,817 : INFO : PROGRESS: processing document #20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:07,913 : INFO : PROGRESS: processing document #30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:08,765 : INFO : PROGRESS: processing document #40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:09,930 : INFO : PROGRESS: processing document #50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:11,526 : INFO : PROGRESS: processing document #60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:12,476 : INFO : PROGRESS: processing document #70000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:13,837 : INFO : PROGRESS: processing document #80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:14,341 : INFO : PROGRESS: processing document #90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:15,467 : INFO : calculating IDF weights for 100000 documents and 767561 features (20122715 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_tfidf = tfidf[train_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_tfidf = tfidf[test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:16,547 : INFO : using serial LSI version on this node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:21:16,549 : INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:24:37,650 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:24:38,864 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:24:38,866 : INFO : 1st phase: constructing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:24:44,538 : INFO : orthonormalizing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:28:20,194 : INFO : 2nd phase: running dense svd on (600, 20000) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:28:32,906 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:28:32,944 : INFO : keeping 500 factors (discarding 6.346% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:45,942 : INFO : processed documents up to #20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:47,283 : INFO : topic #0(14.950): 0.299*\"车型\" + 0.169*\"SUV\" + 0.164*\"发动机\" + 0.158*\"车\" + 0.151*\"汽车\" + 0.138*\"新车\" + 0.129*\"市场\" + 0.124*\"设计\" + 0.118*\"动力\" + 0.111*\"T\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:47,337 : INFO : topic #1(12.308): -0.242*\"车型\" + -0.141*\"发动机\" + 0.141*\"投资\" + -0.135*\"SUV\" + 0.135*\"比赛\" + 0.120*\"公司\" + -0.120*\"新车\" + 0.116*\"球队\" + 0.116*\"市场\" + -0.109*\"车\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:47,389 : INFO : topic #2(10.942): 0.339*\"比赛\" + 0.297*\"球队\" + 0.251*\"球员\" + 0.158*\"赛季\" + 0.136*\"联赛\" + 0.119*\"皇马\" + 0.117*\"球迷\" + -0.103*\"市场\" + -0.103*\"投资\" + 0.100*\"进球\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:47,430 : INFO : topic #3(7.936): -0.278*\"美元\" + 0.240*\"汽车\" + -0.218*\"美联储\" + -0.201*\"加息\" + 0.181*\"公司\" + 0.126*\"企业\" + -0.122*\"指数\" + 0.107*\"新能源\" + -0.101*\"人民币\" + -0.099*\"上涨\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:47,470 : INFO : topic #4(7.655): -0.214*\"热巴\" + -0.159*\"网友\" + -0.141*\"赵丽颖\" + -0.140*\"节目\" + -0.136*\"郑爽\" + -0.132*\"美元\" + -0.122*\"粉丝\" + -0.120*\"鹿晗\" + 0.109*\"基金\" + 0.108*\"公司\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:56,825 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:57,874 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:29:57,875 : INFO : 1st phase: constructing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:30:04,104 : INFO : orthonormalizing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:34:27,997 : INFO : 2nd phase: running dense svd on (600, 20000) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:34:45,703 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:34:45,744 : INFO : keeping 500 factors (discarding 5.750% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:35:51,074 : INFO : merging projections: (767562, 500) + (767562, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:38:10,862 : INFO : keeping 500 factors (discarding 21.767% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:30,869 : INFO : processed documents up to #40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:30,984 : INFO : topic #0(18.547): 0.205*\"手机\" + 0.136*\"市场\" + 0.127*\"车型\" + 0.126*\"公司\" + 0.117*\"小米\" + 0.111*\"产品\" + 0.110*\"汽车\" + 0.109*\"用户\" + 0.103*\"苹果\" + 0.103*\"企业\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:31,044 : INFO : topic #1(13.389): 0.253*\"放入\" + 0.202*\"克\" + 0.179*\"盐\" + 0.163*\"适量\" + 0.154*\"倒入\" + -0.138*\"车型\" + 0.130*\"翻炒\" + 0.126*\"鸡蛋\" + 0.117*\"吃\" + 0.116*\"煮\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:31,097 : INFO : topic #2(13.245): -0.262*\"车型\" + -0.152*\"发动机\" + -0.147*\"SUV\" + 0.133*\"公司\" + -0.130*\"设计\" + -0.128*\"新车\" + -0.124*\"车\" + 0.120*\"企业\" + 0.117*\"投资\" + -0.116*\"放入\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:31,158 : INFO : topic #3(12.654): -0.431*\"手机\" + -0.324*\"小米\" + 0.211*\"车型\" + -0.182*\"三星\" + -0.170*\"华为\" + -0.153*\"苹果\" + 0.121*\"汽车\" + 0.119*\"SUV\" + 0.116*\"发动机\" + -0.115*\"处理器\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:31,203 : INFO : topic #4(11.184): 0.331*\"比赛\" + 0.286*\"球队\" + 0.244*\"球员\" + 0.153*\"赛季\" + 0.132*\"联赛\" + 0.126*\"民警\" + 0.115*\"球迷\" + 0.114*\"皇马\" + 0.096*\"进球\" + 0.095*\"中超\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:43,288 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:44,719 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:44,720 : INFO : 1st phase: constructing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:40:54,384 : INFO : orthonormalizing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:46:17,649 : INFO : 2nd phase: running dense svd on (600, 20000) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:46:35,871 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:46:35,917 : INFO : keeping 500 factors (discarding 5.317% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:47:51,403 : INFO : merging projections: (767562, 500) + (767562, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:50:54,103 : INFO : keeping 500 factors (discarding 19.678% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:18,768 : INFO : processed documents up to #60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:18,890 : INFO : topic #0(22.089): 0.287*\"孩子\" + 0.171*\"宝宝\" + 0.134*\"市场\" + 0.112*\"电影\" + 0.110*\"年\" + 0.109*\"手机\" + 0.108*\"城市\" + 0.103*\"房价\" + 0.100*\"中国\" + 0.099*\"房地产\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:18,955 : INFO : topic #1(18.515): -0.651*\"孩子\" + -0.418*\"宝宝\" + -0.172*\"妈妈\" + -0.120*\"家长\" + -0.113*\"父母\" + 0.104*\"市场\" + 0.094*\"房价\" + 0.093*\"房地产\" + 0.089*\"城市\" + 0.067*\"投资\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:19,002 : INFO : topic #2(15.496): -0.228*\"房价\" + -0.200*\"房地产\" + -0.180*\"城市\" + 0.180*\"手机\" + -0.167*\"孩子\" + 0.141*\"电影\" + 0.140*\"车型\" + -0.132*\"调控\" + -0.129*\"楼市\" + 0.126*\"设计\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:19,051 : INFO : topic #3(14.405): -0.767*\"宝宝\" + 0.473*\"孩子\" + 0.151*\"电影\" + 0.085*\"父母\" + 0.062*\"家长\" + 0.059*\"影片\" + 0.058*\"导演\" + -0.052*\"吃\" + -0.052*\"母乳\" + 0.049*\"票房\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:19,093 : INFO : topic #4(14.086): 0.441*\"电影\" + -0.237*\"孩子\" + -0.189*\"手机\" + 0.172*\"导演\" + 0.169*\"影片\" + 0.153*\"票房\" + -0.137*\"车型\" + -0.124*\"小米\" + 0.117*\"这部\" + 0.115*\"上映\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:30,839 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:32,119 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:32,120 : INFO : 1st phase: constructing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 14:54:39,757 : INFO : orthonormalizing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:00:55,670 : INFO : 2nd phase: running dense svd on (600, 20000) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:01:12,916 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:01:12,967 : INFO : keeping 500 factors (discarding 4.846% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:02:22,358 : INFO : merging projections: (767562, 500) + (767562, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:06:11,700 : INFO : keeping 500 factors (discarding 19.040% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:22,209 : INFO : processed documents up to #80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:22,346 : INFO : topic #0(23.872): 0.217*\"孩子\" + 0.152*\"中国\" + 0.125*\"宝宝\" + 0.121*\"年\" + 0.116*\"美国\" + 0.113*\"市场\" + 0.104*\"月\" + 0.099*\"电影\" + 0.090*\"城市\" + 0.089*\"手机\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:22,412 : INFO : topic #1(18.858): 0.640*\"孩子\" + 0.405*\"宝宝\" + 0.171*\"妈妈\" + 0.117*\"家长\" + 0.114*\"父母\" + -0.091*\"中国\" + -0.090*\"市场\" + -0.078*\"房地产\" + -0.076*\"房价\" + -0.074*\"城市\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:22,470 : INFO : topic #2(16.012): -0.285*\"孩子\" + 0.237*\"星座\" + -0.187*\"宝宝\" + -0.171*\"房价\" + -0.156*\"房地产\" + -0.141*\"城市\" + 0.141*\"电影\" + 0.134*\"爱情\" + 0.106*\"天蝎座\" + 0.105*\"处女座\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:22,522 : INFO : topic #3(15.213): 0.251*\"星座\" + 0.176*\"房价\" + -0.162*\"手机\" + 0.146*\"房地产\" + -0.140*\"宝宝\" + 0.136*\"城市\" + -0.131*\"车型\" + 0.131*\"爱情\" + 0.111*\"天蝎座\" + -0.110*\"设计\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:22,577 : INFO : topic #4(14.884): 0.268*\"美国\" + 0.212*\"特朗普\" + 0.177*\"中国\" + -0.169*\"手机\" + 0.166*\"俄罗斯\" + 0.159*\"航母\" + -0.154*\"宝宝\" + 0.137*\"日本\" + -0.127*\"车型\" + 0.124*\"导弹\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:32,174 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:33,162 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:33,163 : INFO : 1st phase: constructing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:11:38,941 : INFO : orthonormalizing (767562, 600) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:17:14,139 : INFO : 2nd phase: running dense svd on (600, 20000) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:17:24,563 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:17:24,604 : INFO : keeping 500 factors (discarding 5.223% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:18:16,814 : INFO : merging projections: (767562, 500) + (767562, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:22:38,822 : INFO : keeping 500 factors (discarding 15.797% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:28:14,208 : INFO : processed documents up to #100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:28:14,323 : INFO : topic #0(26.036): 0.274*\"孩子\" + 0.146*\"宝宝\" + 0.109*\"中国\" + 0.102*\"男人\" + 0.091*\"年\" + 0.089*\"妈妈\" + 0.087*\"女人\" + 0.084*\"电影\" + 0.083*\"美国\" + 0.082*\"爱\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:28:14,384 : INFO : topic #1(20.091): -0.471*\"孩子\" + -0.273*\"宝宝\" + 0.140*\"中国\" + -0.136*\"妈妈\" + 0.124*\"市场\" + 0.106*\"美国\" + 0.101*\"年\" + 0.099*\"房地产\" + 0.097*\"房价\" + 0.097*\"城市\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:28:14,441 : INFO : topic #2(17.516): -0.469*\"孩子\" + -0.364*\"宝宝\" + 0.229*\"男人\" + 0.180*\"女人\" + 0.169*\"星座\" + 0.164*\"狗狗\" + 0.156*\"爱情\" + 0.126*\"感情\" + -0.110*\"妈妈\" + 0.109*\"爱\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:28:14,493 : INFO : topic #3(16.400): 0.673*\"狗狗\" + 0.336*\"主人\" + -0.191*\"孩子\" + -0.172*\"男人\" + 0.163*\"狗\" + 0.150*\"宝宝\" + -0.138*\"女人\" + 0.115*\"喵\" + -0.105*\"星座\" + 0.105*\"猫咪\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-03 15:28:14,529 : INFO : topic #4(15.627): -0.284*\"狗狗\" + 0.280*\"宝宝\" + -0.221*\"孩子\" + -0.188*\"房价\" + -0.163*\"房地产\" + -0.152*\"城市\" + 0.141*\"手机\" + -0.133*\"主人\" + 0.116*\"车型\" + 0.108*\"设计\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(train_corpus_tfidf, id2word=dictionary, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = similarities.MatrixSimilarity(lsi[train_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.concat([train, test])\n",
    "clean_data[\"category\"] = clean_data[\"category\"].astype(\"category\")\n",
    "clean_data[\"category_encoded\"] = clean_data[\"category\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_data[:sample_num * len(categories)]\n",
    "test = clean_data[sample_num * len(categories):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n(422067, 5)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = test[\"cutwords\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_num = 0\n",
    "# top1_correct_num = 0\n",
    "# top3_correct_num = 0\n",
    "# top5_correct_num = 0\n",
    "# top10_correct_num = 0\n",
    "# \n",
    "# for i in range(len(test)):\n",
    "#     try:\n",
    "#         query_bow = dictionary.doc2bow(query[i].split())\n",
    "#         query_lsi = lsi[query_bow]\n",
    "#         sims = index[query_lsi]\n",
    "#         sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "#         sort_sims = sort_sims[:100]\n",
    "#         predict = defaultdict(int)\n",
    "#         for sort_sim in sort_sims:\n",
    "#             predict_cat = train.iloc[sort_sim[0]][\"category\"]\n",
    "#             predict[predict_cat] += sort_sim[1]\n",
    "#         sorted_predict = sorted(predict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#         \n",
    "#         true_cat = test.iloc[i][\"category\"]\n",
    "#         predict_cats = [item[0] for item in sorted_predict]\n",
    "#         if true_cat == predict_cats[0]:\n",
    "#             top1_correct_num += 1\n",
    "#         else:\n",
    "#             with open(\"logs/bad_case_top1.log\", \"ab\") as f:\n",
    "#                 f.write(\"{},{},{}\\n\".format(test.iloc[i][\"id\"], \" \".join(predict_cats), test.iloc[i][\"category\"]))\n",
    "#         if true_cat in predict_cats[:3]:\n",
    "#             top3_correct_num += 1\n",
    "#         if true_cat in predict_cats[:5]:\n",
    "#             top5_correct_num += 1\n",
    "#         if true_cat in predict_cats[:10]:\n",
    "#             top10_correct_num += 1\n",
    "#         else:\n",
    "#             with open(\"logs/bad_case_top10.log\", \"ab\") as f:\n",
    "#                 f.write(\"{},{},{}\\n\".format(test.iloc[i][\"id\"], \" \".join(predict_cats), test.iloc[i][\"category\"]))\n",
    "#         total_num += 1\n",
    "#         if i % 10 == 0:\n",
    "#             with open(\"logs/run.log\", \"ab\") as f:\n",
    "#                 f.write(\"{},{},{},{},{},{}\\n\".format(i, top1_correct_num, top3_correct_num, top5_correct_num, top10_correct_num, total_num))\n",
    "#     except:\n",
    "#         with open(\"logs/exception.log\", \"ab\") as f:\n",
    "#             f.write(\"{}\\n\".format(i))\n",
    "#         continue\n",
    "#     # if i == 100:\n",
    "#     #     break\n",
    "# print top1_correct_num\n",
    "# print top3_correct_num\n",
    "# print top5_correct_num\n",
    "# print top10_correct_num\n",
    "# print total_num\n",
    "# print float(top1_correct_num) * 100 / total_num\n",
    "# print float(top3_correct_num) * 100 / total_num\n",
    "# print float(top5_correct_num) * 100 / total_num\n",
    "# print float(top10_correct_num) * 100 / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_case = pd.read_csv(\"logs/bad_case_top1.log\", names=[\"id\", \"predict_category\", \"true_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = test[\"category\"].value_counts()\n",
    "# b = bad_case[\"true_category\"].value_counts()\n",
    "# for cat in categories:\n",
    "#     cat = cat.encode(\"utf8\")\n",
    "#     print \"{}: {}\".format(cat, float(b[cat]) * 100 / a[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lsi = lsi[train_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [[t2[1] for t2 in t1] for t1 in train_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[\"category_encoded\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lsi = lsi[test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = [[t2[1] for t2 in t1] for t1 in test_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test[\"category_encoded\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n[]\n100000\n"
     ]
    }
   ],
   "source": [
    "print len(train_x)\n",
    "if len(train_x) != len(train_y):\n",
    "    print \"train x and y not match\"\n",
    "else:\n",
    "    idx2del = []\n",
    "    for i in range(len(train_x)):\n",
    "        if not train_x[i] or (train_y[i] != 0 and not train_y[i]) or len(train_x[i]) != num_topics:\n",
    "            idx2del.append(i)\n",
    "    print idx2del\n",
    "    for i in sorted(idx2del, reverse=True):\n",
    "        del train_x[i]\n",
    "        del train_y[i]\n",
    "    print len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = svm.SVC()\n",
    "# clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(C=0.1, n_jobs=-1)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(clf, \"svm_{}feas.pkl\".format(num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32805, 72842, 113020, 117655, 127501, 128237, 129809, 137487, 144854, 146245, 146668, 147357, 147906, 148524, 149253, 149561, 149811, 150512, 152915, 158545, 158548, 203952, 257723, 395217, 410622]\n422042\n"
     ]
    }
   ],
   "source": [
    "print len(test_x)\n",
    "if len(test_x) != len(test_y):\n",
    "    print \"test x and y not match\"\n",
    "else:\n",
    "    idx2del = []\n",
    "    for i in range(len(test_x)):\n",
    "        if not test_x[i] or (test_y[i] != 0 and not test_y[i]) or len(test_x[i]) != num_topics:\n",
    "            idx2del.append(i)\n",
    "    print idx2del\n",
    "    for i in sorted(idx2del, reverse=True):\n",
    "        del test_x[i]\n",
    "        del test_y[i]\n",
    "    print len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.1612303989\n"
     ]
    }
   ],
   "source": [
    "correct_num = 0\n",
    "for i in range(len(test_x)):\n",
    "    predict = clf.predict([test_x[i]])\n",
    "    if predict[0] == test_y[i]:\n",
    "        correct_num += 1\n",
    "    if i % 100 == 0:\n",
    "        with open(\"logs/run.log\", \"ab\") as f:\n",
    "            f.write(\"{}, {}\\n\".format(i, correct_num))\n",
    "print float(correct_num) * 100 / len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.1612303989\n"
     ]
    }
   ],
   "source": [
    "print float(correct_num) * 100 / len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.096\n"
     ]
    }
   ],
   "source": [
    "correct_num = 0\n",
    "for i in range(len(train_x)):\n",
    "    predict = clf.predict([train_x[i]])\n",
    "    if predict[0] == train_y[i]:\n",
    "        correct_num += 1\n",
    "    if i % 100 == 0:\n",
    "        with open(\"logs/run.log\", \"ab\") as f:\n",
    "            f.write(\"{}, {}\\n\".format(i, correct_num))\n",
    "print float(correct_num) * 100 / len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.096\n"
     ]
    }
   ],
   "source": [
    "print float(correct_num) * 100 / len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}